import pandas as pd
import seaborn as sns
import seaborn.objects as so
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder


df = pd.read_csv('data/clean_folder/out.csv')
df.head()



df.info()





categorical_types = ['object']  # the categorical types in Pandas
categorical_columns = df.select_dtypes(include=categorical_types).columns.tolist()
# Isolate the numeric features in group for easier processing
numeric_types = ['float16','float32', 'float64', 'int16', 'int32','int64']   # the numeric types in Pandas
numeric_columns = df.select_dtypes(include=numeric_types).columns.tolist()# data['month'] = data['month'].apply(month_num)
numeric_columns.remove('y')


df[numeric_columns].describe()



pd.DataFrame(# mean of 0 and std of 1 but ranges are different
    StandardScaler().fit_transform(df[numeric_columns]), columns=numeric_columns).describe()


# divide the dataset into training, validation and test set using sklearn train_test_split
from sklearn.model_selection import train_test_split
df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=11)
df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=11)


print(f'Full Training Set: ', df_train_full.shape)
print(f'Training Set: ', df_train.shape)
print(f'Validation Set: ', df_val.shape)
print(f'Test Set: ', df_test.shape)


y_train = df_train['y']
y_val = df_val['y']


del df_train['y']
del df_val['y']


X = df_train.copy()


X.head()


X.dtypes


(
    so.Plot(data, x="month")
    .add(so.Bar(), so.Hist())
)


categorical_columns


numeric_columns


X = X[categorical_columns + numeric_columns]









df.head()


num_attribs = list(numeric_columns)
cat_attribs = list(categorical_columns)


full_pipeline = ColumnTransformer([
    ('num', StandardScaler(), num_attribs),
    ('cat', OneHotEncoder(), cat_attribs),
])


print(df_train.shape)
print(df_val.shape)
print(df_test.shape)


y_train = y_train
y_val = y_val





df_val.head()


X_train = full_pipeline.fit_transform(df_train)
#X_train = full_pipeline.transform(df_train)
X_val = full_pipeline.transform(df_val)


from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)


y_train.value_counts()


y_pred = dt.predict_proba(X_train)[:, 1]
roc_auc_score(y_train, y_pred)


y_pred = dt.predict_proba(X_val)[:, 1]
roc_auc_score(y_val, y_pred)


from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=10)
rf.fit(X_train, y_train)


y_pred = rf.predict_proba(X_val)[:, 1]
roc_auc_score(y_val, y_pred)


rf = RandomForestClassifier(n_estimators=10, random_state=3)
rf.fit(X_train, y_train)
y_pred = rf.predict_proba(X_val)[:, 1]
roc_auc_score(y_val, y_pred)


df_train.head()


df_train.columns


all_aucs = {}

for depth in [10, 20, 40]:
    print('depth: %s' % depth)
    aucs = []

    for i in range(10, 201, 10):
        rf = RandomForestClassifier(n_estimators=i, max_depth=depth, random_state=1)
        rf.fit(X_train, y_train)
        y_pred = rf.predict_proba(X_val)[:, 1]
        auc = roc_auc_score(y_val, y_pred)
        print('%s -> %.3f' % (i, auc))
        aucs.append(auc)
    
    all_aucs[depth] = aucs
    print()


plt.figure(figsize=(6, 4))

num_trees = list(range(10, 201, 10))

plt.plot(num_trees, all_aucs[10], label='depth=10', color='black', linestyle='dotted')
plt.plot(num_trees, all_aucs[20], label='depth=20', color='black', linestyle='dashed')
plt.plot(num_trees, all_aucs[40], label='depth=40', color='black', linestyle='solid')
    
plt.xticks(range(0, 201, 50))
plt.legend()

plt.title('Number of trees vs AUC')
plt.xlabel('Number of trees')
plt.ylabel('AUC')

# plt.savefig('ch06-figures/06_random_forest_n_estimators_depth.svg')

plt.show()


from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC


log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC(probability=True)


voting_clf = VotingClassifier(
estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
voting='soft')
voting_clf.fit(X_train, y_train)


from sklearn.metrics import accuracy_score
for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_val)
    print(clf.__class__.__name__, accuracy_score(y_val, y_pred))


from sklearn.feature_extraction import DictVectorizer
train_dict = df_train[categorical_columns + numeric_columns].to_dict(orient='records')
val_dict = df_val[categorical_columns + numeric_columns].to_dict(orient='records')
test_dict = df_test[categorical_columns + numeric_columns].to_dict(orient='records')
dv = DictVectorizer(sparse=False)
dv.fit(train_dict)


X_train = dv.transform(train_dict)


from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=10)
rf.fit(X_train, y_train)


y_pred = rf.predict_proba(X_val)[:, 1]
roc_auc_score(y_val, y_pred)


from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import mutual_info_classif, SelectKBest
from sklearn.model_selection import GridSearchCV


numeric_transformer = make_pipeline(StandardScaler())
categorical_transformer = make_pipeline(OneHotEncoder(handle_unknown='ignore'))
preprocessor = make_column_transformer((numeric_transformer, num_attribs),(categorical_transformer, cat_attribs))
pipeline = make_pipeline(preprocessor, RandomForestClassifier())
pipeline


pipeline.fit(df_train, y_train)
score = pipeline.score(df_val, y_val)
print(score)


import joblib
# Save the pickled object to disk.
joblib.dump(pipeline, 'pipeline.pkl')


import pickle

with open("pipeline2.pkl", "wb") as f:
    pickle.dump(pipeline, f)


with open("pipeline2.pkl", "rb") as f:
    model1 = pickle.load(f)


model1.predict_proba(df_test)[0]


prediction = pipeline.predict_proba(df_train)[:, 1]
prediction[0]


from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
def learning_func(model):
    numeric_transformer = make_pipeline(StandardScaler())
    categorical_transformer = make_pipeline(OneHotEncoder(handle_unknown='ignore'))
    preprocessor = make_column_transformer((numeric_transformer, num_attribs),(categorical_transformer, cat_attribs))
    pipeline = make_pipeline(preprocessor, RandomForestClassifier())
    fitted_model = pipeline.fit(df_train, y_train)
    print("Accuracy Score:", fitted_model.score(df_val, y_val))
    model_preds = fitted_model.predict(df_val)
    print(classification_report(y_val, model_preds))
    print(confusion_matrix(y_val, model_preds))



learning_func(RandomForestClassifier(random_state=123, n_estimators=20))






